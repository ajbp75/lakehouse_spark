# ğŸ“¦ Sonar â€“ Teste TÃ©cnico Engenharia de Dados (Lakehouse)

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um mini fluxo **Lakehouse (Bronze â†’ Silver â†’ Gold)** utilizando **PySpark (equivalente ao Microsoft Fabric)**.

O objetivo foi:

* Construir modelagem dimensional mÃ­nima (Star Schema)
* Garantir idempotÃªncia do pipeline
* Implementar Data Quality (DQ)
* Gerar mÃ©tricas via SQL
* Produzir logs simples de execuÃ§Ã£o

O pipeline pode ser executado mÃºltiplas vezes sem gerar duplicaÃ§Ãµes na camada Gold.

---

# ğŸ— Arquitetura

```
data (raw CSVs)
   â†“
Bronze (ingestÃ£o)
   â†“
Silver (limpeza + deduplicaÃ§Ã£o + tratamento)
   â†“
Gold (modelo dimensional final)
   â†“
DQ + Logs
   â†“
SQL Metrics
```

---

# ğŸ¥‰ Bronze Layer

ResponsÃ¡vel apenas por ingestÃ£o dos arquivos CSV:

* `customers.csv`
* `work_orders.csv`
* `parts_sales.csv`

Nenhuma transformaÃ§Ã£o Ã© aplicada nessa camada.

---

# ğŸ¥ˆ Silver Layer

Nesta camada foram aplicadas as regras de negÃ³cio.

## ğŸ”¹ DeduplicaÃ§Ã£o

| Tabela      | Chave         | CritÃ©rio                |
| ----------- | ------------- | ----------------------- |
| customers   | customer_id   | manter maior created_at |
| work_orders | work_order_id | manter maior updated_at |
| parts_sales | sale_id       | manter maior updated_at |

A deduplicaÃ§Ã£o Ã© determinÃ­stica e garante consistÃªncia entre execuÃ§Ãµes.

---

## ğŸ”¹ Tratamento de Nulos

### fact_work_order

* `order_date` nulo â†’ removido (necessÃ¡rio para anÃ¡lises temporais)
* `customer_id` nulo â†’ substituÃ­do por `-1` (UNKNOWN)

### fact_parts_sales

* `unit_price` nulo â†’ substituÃ­do por `0`
* `work_order_id` nulo â†’ removido

---

## ğŸ”¹ Tratamento de Ã“rfÃ£os

### work_orders sem customer correspondente

Foi criado um registro especial na dimensÃ£o:

```
customer_id = -1
customer_name = "UNKNOWN"
```

Isso preserva o fato e mantÃ©m integridade referencial.

### parts_sales sem work_order correspondente

Registros removidos por inconsistÃªncia estrutural.

---

# ğŸ¥‡ Gold Layer (Star Schema)

## ğŸ“Œ dim_customer.csv

* customer_id
* customer_name
* segment
* state
* * registro UNKNOWN

## ğŸ“Œ fact_work_order.csv

* work_order_id
* customer_id
* order_date
* status
* labor_hours
* labor_cost

## ğŸ“Œ fact_parts_sales.csv

* sale_id
* work_order_id
* sku
* quantity
* unit_price
* total_price
* sale_date

---

# ğŸ“Š Data Quality (DQ)

Arquivo: `dq/dq_results.csv`

Checks implementados:

1. Taxa de nulos em colunas crÃ­ticas
2. Taxa de duplicidade por chave
3. Taxa de Ã³rfÃ£os (fato â†’ dimensÃ£o)

Cada check contÃ©m:

* check_name
* table_name
* metric_value
* threshold
* status (PASS/FAIL)
* details

---

# ğŸ“‹ Log de ExecuÃ§Ã£o

Arquivo: `dq/pipeline_runs.csv`

Campos:

* run_id
* started_at
* ended_at
* duration_seconds
* rows_dim_customer
* rows_fact_work_order
* rows_fact_parts_sales

---

# ğŸ§® SQL Metrics

Arquivo: `sql/metrics.sql`

Consultas incluÃ­das:

1. Receita total de peÃ§as por cliente (Ãºltimos 90 dias)
2. Ordens por status por mÃªs
3. Ticket mÃ©dio de peÃ§as por ordem

---

# ğŸ” IdempotÃªncia

O pipeline:

* Sempre lÃª os dados originais
* Deduplica deterministicamente
* Escreve os resultados finais em modo overwrite

Rodar mÃºltiplas vezes nÃ£o gera duplicaÃ§Ãµes.

---

# â–¶ï¸ Como Executar

1. Criar ambiente virtual:

```
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

2. Executar pipeline:

```
python pipeline.py
```

3. Os arquivos finais serÃ£o gerados dentro de:

```
submission_<SEU_NOME>/
```

---

# âš ï¸ LimitaÃ§Ãµes

* NÃ£o implementa carga incremental (apenas full refresh)
* NÃ£o hÃ¡ testes automatizados (priorizaÃ§Ã£o devido ao tempo do teste)
* NÃ£o hÃ¡ versionamento de schema

---

# ğŸš€ PrÃ³ximos Passos (em ambiente real)

* Implementar carga incremental com merge
* Adicionar testes automatizados (pytest + validaÃ§Ãµes de DQ)
* OrquestraÃ§Ã£o (Airflow/Fabric Pipeline)
* Monitoramento estruturado
* Versionamento de schema

---

# ğŸ§  ObservaÃ§Ã£o Final

A soluÃ§Ã£o foi construÃ­da priorizando:

* Clareza arquitetural
* Integridade referencial
* ConsistÃªncia de regras
* Reprodutibilidade
* GovernanÃ§a bÃ¡sica de dados
